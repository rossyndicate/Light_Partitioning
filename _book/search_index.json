[["index.html", "1 Light Partitioning", " 1 Light Partitioning This bookdown documents the harmonization process for parameters from the Water Quality Portal. "],["download-process.html", "2 Download process", " 2 Download process 2.0.1 Catalogue existing data The download process begins by cataloging the existing data that is available. We start out using the following parameters and parameter codes for retrieving data: chlorophyll: “Chlorophyll” “Chlorophyll A” “Chlorophyll a” “Chlorophyll a (probe relative fluorescence)” “Chlorophyll a (probe)” “Chlorophyll a - Periphyton (attached)” “Chlorophyll a - Phytoplankton (suspended)” “Chlorophyll a, corrected for pheophytin” “Chlorophyll a, free of pheophytin” “Chlorophyll a, uncorrected for pheophytin” “Chlorophyll b” “Chlorophyll c” “Chlorophyll/Pheophytin ratio” secchi: “Depth, Secchi disk depth” “Depth, Secchi disk depth (choice list)” “Secchi Reading Condition (choice list)” “Secchi depth” “Water transparency, Secchi disc” doc: “Organic carbon” “Total carbon” “Hydrophilic fraction of organic carbon” “Non-purgeable Organic Carbon (NPOC)” tss: “Total suspended solids” “Total Suspended Particulate Matter” 2.0.2 Maps of data spread within the contiguous US: Maps are presented below with counts of records across a grid. The grid is how records are grouped in download requests to the Water Quality Portal. Florida is mapped separately because of its high concentration of values. Note: 1) the top four grid cells in the FL map are included in both maps, 2) the counts here are for raw data that are not filtered for simultaneous records. Code # Combine counts in each grid_id with the grid polygons grid_counts &lt;- left_join(x = global_grid, y = site_counts %&gt;% count(grid_id), by = c(&quot;id&quot; = &quot;grid_id&quot;)) %&gt;% st_transform(crs = 9311) state_selection &lt;- states(progress_bar = FALSE) %&gt;% filter(!NAME %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;, &quot;American Samoa&quot;, &quot;Guam&quot;, &quot;Puerto Rico&quot;, &quot;United States Virgin Islands&quot;, &quot;Commonwealth of the Northern Mariana Islands&quot;)) %&gt;% st_transform(crs = 9311) ## Retrieving data for the year 2020 Code non_fl_map &lt;- ggplot() + geom_sf(data = grid_counts %&gt;% filter(!id %in% c(#10847:10850, 10668, 10669, 10670, 10489, 10490, 10491, 10309, 10310)), aes(fill = n)) + geom_sf(data = state_selection %&gt;% filter(NAME != &quot;Florida&quot;), fill = NA, color = &quot;white&quot;) + # geom_sf_text(data = grid_counts, aes(label = id)) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(state_selection)[,&quot;X&quot;]), max(st_coordinates(state_selection)[,&quot;X&quot;])), ylim = c(min(st_coordinates(state_selection)[,&quot;Y&quot;]), max(st_coordinates(state_selection)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;) + theme_bw() non_fl_map Code fl_states &lt;- state_selection %&gt;% filter(NAME %in% c(&quot;Florida&quot;, &quot;Georgia&quot;, &quot;Alabama&quot;)) fl_map &lt;- ggplot() + geom_sf(data = grid_counts %&gt;% filter(id %in% c(10847:10850, 10668, 10669, 10670, 10489, 10490, 10491, 10309, 10310)), aes(fill = n)) + geom_sf(data = fl_states, fill = NA, color = &quot;black&quot;) + # geom_sf_label(data = grid_counts, aes(label = id)) + xlab(NULL) + ylab(NULL) + coord_sf(xlim = c(min(st_coordinates(fl_states)[,&quot;X&quot;]), max(st_coordinates(fl_states)[,&quot;X&quot;])), ylim = c(min(st_coordinates(fl_states)[,&quot;Y&quot;]), max(st_coordinates(fl_states)[,&quot;Y&quot;]))) + scale_fill_viridis_c(&quot;Record count&quot;) + theme_bw() fl_map "],["pre-harmonization-decision-process.html", "3 Pre-harmonization decision process", " 3 Pre-harmonization decision process 3.0.1 0. Initial dataset At the start of the preharmonization process the WQP dataset contains 15.07 million rows. 3.0.2 1. Remove duplicates The first filtering step is to remove any duplicated records present in the dataset. This results in dropping 3.02 million rows for a record count of 12.06 million. 3.0.3 2. Missing results Next missing results are dropped from the dataset. 499.56 thousand rows are dropped, resulting in a final count of 11.56 million. 3.0.4 2. Filter status We next filter the ResultStatusIdentifier column to include only the following statuses: \"Accepted\" \"Final\" \"Historical\" \"Validated\" \"Preliminary\" NA 43.53 thousand rows are dropped from the dataset leaving it with 11.51 million remaining. 3.0.5 3. Filter media Next the ActivityMediaSubdivisionName is filtered to only include only the following media: \"Surface Water\" \"Water\" \"Estuary\" NA 24.29 thousand rows are dropped and 11.49 million remain. 3.0.5.1 4. Location type The final step in pre-harmonization is filtering out any ResolvedMonitoringLocationTypeName that is not \"Estuary\", \"Lake, Reservoir, Impoundment\", or \"Stream\". After dropping 124.4 thousand rows the pre-harmonization dataset is complete with 11.36 million. "],["chlorophyll-harmonization-process-strict-version.html", "4 Chlorophyll harmonization process (strict version)", " 4 Chlorophyll harmonization process (strict version) 4.0.1 0. Initial dataset After the preharmonization process the chlorophyll-only WQP dataset contains 3.41 million rows. 4.0.2 1. Filter for water media The first step in chla harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The final row count after this is 3.41 million. 4.0.3 2. Keep only chlorophyll parameters The next step is to ensure that there are only parameter names related to chlorophyll a in the dataset. We retain the following values: Chlorophyll a Chlorophyll a (probe relative fluorescence) Chlorophyll a, corrected for pheophytin Chlorophyll a (probe) Chlorophyll a, free of pheophytin Chlorophyll a - Phytoplankton (suspended) 1.29 million rows are removed and 2.12 million rows remain. 4.0.4 3. Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, and ResultMeasureValue. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no data\". Specific target text varies by column. 84.07 thousand rows are removed and 2.04 million rows remain. 4.0.5 4. Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.04 million. 4.0.6 5. Clean approximate values Step 5 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.04 million. 4.0.7 6. Clean values with “greater than” data Step 6 is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.04 million. 4.0.8 7. Harmonize record units The next step in chla harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into micrograms/L: units conversion mg/l 1e+03 mg/L 1e+03 ppm 1e+03 ug/l 1e+00 ug/L 1e+00 mg/m3 1e+00 ppb 1e+00 mg/cm3 1e+06 ug/ml 1e+03 mg/ml 1e+06 ppt 1e+06 This should not result in a change in rows but we still check: 1.271^{5} rows are removed. The final row count after this is 1.91 million. 4.0.9 8. Clean depth data The ActivityDepthHeightMeasure.MeasureValue column is the site of our next harmonization step. We want to both harmonize the units (ActivityDepthHeightMeasure.MeasureUnitCode) used in the dataset and set limits for depths from which we’ll accept data. We’ll use a conversion table as with the previous units harmonization step: depth_units depth_conversion in 0.0254 ft 0.3048 feet 0.3048 cm 0.0100 m 1.0000 meters 1.0000 Once the units have been standardized we’ll make sure that the numeric depth value is within +/-2m OR the raw character version indicates something similar. We also keep NA depths to avoid losing a ton of records. This is how it’s coded: Code converted_units_chla %&gt;% left_join(x = ., y = depth_unit_conversion_table, by = c(&quot;sample_depth_unit&quot; = &quot;depth_units&quot;)) %&gt;% mutate(harmonized_depth_value = as.numeric(sample_depth) * depth_conversion, harmonized_depth_unit = &quot;m&quot;) %&gt;% filter(abs(harmonized_depth_value) &lt;= 2 | sample_depth %in% c(&quot;0-2&quot;, &quot;0-0.5&quot;)| is.na(sample_depth)) Through our depth filtering we lose 121.13 thousand rows and have 1.79 million remaining. 4.0.10 9. Filter based on analytical method Our next step is to aggregate chla analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. We accomplish this using an external match table csv file that is joined to the dataset. Methods that are NA for their aggregated grouping or which are \"unlikely\" to be accurate methods are dropped. This process drops 2648 rows leaving 1.79 million remaining. 4.0.11 10. Filter based on fraction type The final step in our chla harmonization is filtering based on the ResultSampleFractionText column. We assign fractions into two categories based on whether the fraction text makes sense or not and then retain only those records that have a fraction with \"Makes sense\". Fractions included in this are \"Non-Filterable (Particle)\", \"Suspended\", \"Non-filterable\", \"&lt;Blank&gt;\", and \"Acid Soluble\". This process drops 116.99 thousand rows leaving 1.67 million remaining in the final harmonized chla dataset. "],["doc-harmonization-process-strict-version.html", "5 DOC harmonization process (strict version)", " 5 DOC harmonization process (strict version) 5.0.1 0. Initial dataset After the preharmonization process the DOC-only WQP dataset contains 2.15 million rows. 5.0.2 1. Filter for water media The first step in DOC harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The final row count after this is 2.15 million. 5.0.3 2. Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, ResultMeasureValue, and ResultDetectionConditionText. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no data\". Specific target text varies by column. 72.25 thousand rows are removed and 2.08 million rows remain. 5.0.4 3. Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.08 million. 5.0.5 4. Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 2.08 million. 5.0.6 5. Harmonize record units The next step in doc harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into milligrams/L: units conversion mg/L 1.000e+03 mg/l 1.000e+03 ppm 1.000e+03 ug/l 1.000e+00 ug/L 1.000e+00 mg/m3 1.000e+00 ppb 1.000e+00 mg/cm3 1.000e+06 ug/ml 1.000e+03 mg/ml 1.000e+06 ppt 1.000e-06 umol/L 6.008e+01 We also limit values to less than 50 mg/L to ensure realistic data. 47.11 thousand rows are removed. The final row count after this is 2.03 million. 5.0.7 6. Filter based on analytical method Our next step is to aggregate doc analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. Methods that were NA or were grouped as \"Ambiguous\" or \"Carbonaceous Analyzer\" are dropped. This process drops 1032.73 million rows leaving 1 million remaining. 5.0.8 7. Filter based on fraction type The final step in our doc harmonization is filtering based on the ResultSampleFractionText column. We keep records with the following values in this column: \"Dissolved\", \"Filtered, lab\", \"Filterable\", \"Filtered, field\". This process drops 657.28 thousand rows leaving 0.34 million remaining in the final harmonized doc dataset. "],["secchi-harmonization-process-strict-version.html", "6 Secchi harmonization process (strict version)", " 6 Secchi harmonization process (strict version) 6.0.1 0. Initial dataset After the preharmonization process the secchi-only WQP dataset contains 2.57 million rows. 6.0.2 1. Filter for water media The first step in secchi harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The row count after this is 2.57 million. 6.0.3 2. Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, and ResultMeasureValue. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no secchi\". Specific target text varies by column. 19.46 thousand rows are removed and 2.55 million rows remain. 6.0.4 3. Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The row count after this is 2.55 million. 6.0.5 4. Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The row count after this is 2.55 million. 6.0.6 5. Clean values with “greater than” data Step 5 is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The row count after this is 2.55 million. 6.0.7 6. Harmonize record units The next step in secchi harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into meters: units conversion m 1.0000 ft 0.3048 cm 0.0100 in 0.0254 mm 0.0010 mi 1609.3400 We also limit values to less than 15m to ensure realistic data. 9254 rows are removed. The row count after this is 2.54 million. 6.0.8 7. Filter based on analytical method Our next step is to aggregate secchi analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. We accomplish this using an external match table csv file that is joined to the dataset. Methods that are coded \"unlikely\" to be accurate methods are dropped. This process drops 288 rows leaving 2.54 million remaining. 6.0.9 8. Filter based on fraction type The next step in our secchi harmonization is filtering based on the ResultSampleFractionText column. We assign fractions into two categories based on whether the fraction text makes sense or not and then retain only those records that have a fraction with \"Makes sense\". Fractions included in this are NA, \"Total\", \" \", \"None\", \"Unfiltered\", \"Field\". This process drops 94.01 thousand rows leaving 2.45 million remaining in the dataset. 6.0.10 9. Filter based on sample method We next filter based on the sample method, or SampleCollectionMethod.MethodName column. As with analytical methods, we accomplish this using an external match table csv file that is joined to the dataset. Methods that are coded \"unlikely\" to be accurate methods are dropped. This process drops 16.75 thousand rows leaving 2.43 million remaining. 6.0.11 10. Filter based on collection equipment Finally, we examine the collection equipment (SampleCollectionEquipmentName) column to check for equipment that indicates non-secchi measurements. Once again we use an external match table csv file that is joined to the dataset. Equipment types that are coded \"unlikely\" to be accurate to secchi are dropped. This process drops 55.93 thousand rows leaving 2.38 million remaining. "],["tss-harmonization-process-strict-version.html", "7 TSS harmonization process (strict version)", " 7 TSS harmonization process (strict version) 7.0.1 0. Initial dataset After the preharmonization process the tss-only WQP dataset contains 3.23 million rows. 7.0.2 1. Filter for water media The first step in TSS harmonization is to ensure that the media type for the data is \"water\" or \"Water\". This should just be a precautionary step: 0 rows are removed. The final row count after this is 3.23 million. 7.0.3 2. Remove fails and other missing data In this step we filter out records based on indications that they have failed data for some reason. We screen the following columns: ActivityCommentText, ResultLaboratoryCommentText, ResultCommentText, ResultMeasureValue, and ResultDetectionConditionText. Examples of text that results in a dropped record includes (but is not limited to): \"fail\", \"suspect\", \"error\", \"beyond accept\", \"interference\", \"questionable\", \"problem\", \"violation\", \"rejected\", \"no data\". Specific target text varies by column. 151.01 thousand rows are removed and 3.08 million rows remain. 7.0.4 3. Clean MDLs In this step method detection limits (MDLs) are used to clean up the reported values. When a numeric value is missing for the data record (i.e., NA or text that became NA during an as.numeric call) we check for non-detect language in the ResultLaboratoryCommentText, ResultCommentText, ResultDetectionConditionText, and ResultMeasureValue columns. This language can be \"non-detect\", \"not detect\", \"non detect\", \"undetect\", or \"below\". If non-detect language exists then we use the DetectionQuantitationLimitMeasure.MeasureValue column for the MDL, otherwise if there is a &lt; and a number in the ResultMeasureValue column we use that number instead. We then use a random number between 0 and 0.5 * MDL as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 3.08 million. 7.0.5 4. Clean approximate values Step 4 involves a similar process as for MDL cleaning. We flag “approximated” values in the dataset. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL cleaning The original column text contained a number Any of ResultLaboratoryCommentText, ResultCommentText, or ResultDetectionConditionText match this regular expression, ignoring case: \"result approx|RESULT IS APPROX|value approx\" We then use the approximate value as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 3.08 million. 7.0.6 5. Clean values with “greater than” data Step 5 is similar to the MDL and approximate value cleaning processes, and follows the approximate cleaning process most closely. The goal is to clean up values that were entered as “greater than” some value. The ResultMeasureValue column gets checked for all three of the following conditions: Numeric-only version of the column is still NA after MDL &amp; approximate cleaning The original column text contained a number The original column text contained a &gt; We then use the “greater than” value (without &gt;) as the record’s value moving forward. This should not result in a change in rows but we still check: 0 rows are removed. The final row count after this is 3.08 million. 7.0.7 6. Harmonize record units The next step in TSS harmonization is working with the units of the WQP records. These can vary widely. We create the following conversion table, which is used to translate units provided in WQP into milligrams/L: units conversion mg/L 1.000e+03 mg/l 1.000e+03 ppm 1.000e+03 ug/l 1.000e+00 ug/L 1.000e+00 mg/m3 1.000e+00 ppb 1.000e+00 mg/cm3 1.000e+06 ug/ml 1.000e+03 mg/ml 1.000e+06 ppt 1.000e-06 umol/L 6.008e+01 g/l 1.000e+06 We also limit values to less than 1000 mg/L to ensure realistic data. 133.68 thousand rows are removed. The final row count after this is 2.95 million. 7.0.8 7. Filter based on analytical method Our next step is to aggregate TSS analytical methods into groups and then filter out methods that may have been erroneously added, were unclear, or which don’t meet our needs. Methods that were grouped as \"Ambiguous\" or \"Nonsensical\" are dropped. This process drops 1508.86 million rows leaving 1.44 million remaining. 7.0.9 8. Filter based on fraction type The final step in our TSS harmonization is filtering based on the ResultSampleFractionText column. We drop records with the following values in this column: \"Fixed“, \"Volatile\", \"Dissolved\", or \"Acid Soluble\". This process drops 0.15 thousand rows leaving 1.44 million remaining in the final harmonized TSS dataset. "],["light-partitioning-modeling-report.html", "8 Light partitioning modeling report", " 8 Light partitioning modeling report 8.0.0.1 Dataset options for simultaneous records: Code # MR&#39;s original simul &lt;- params$simul # Our attempt at recreating MR&#39;s data simultaneous_data &lt;- params$simultaneous_data # A stricter version simultaneous_data_strict &lt;- params$simultaneous_data_strict 8.0.1 Overview of simultaneous records 8.0.1.1 Differences in number of rows: The original simultaneous Light Partitioning dataset based on AquaSat downloads and MR’s scripts has 100,853 records. Our attempt at achieving a similar number of rows using the current version of the WQP harmonization workflow contains 55,675 records. A stricter version of our dataset contains 25,588 records. Most decisions regarding harmonization choices and filtering for the two more current datasets can be found in 3_harmonize/src/clean_wqp_data.R, 3_harmonize/src/harmonize_[param].R, and similarly named functions with _strict appended for the version used on the stricter dataset. The stricter of our two datasets contains a few more filters than the other datasets: While cleaning the raw WQP data for later use, missing rows are removed. This likely has little to no effect on the dataset given that downstream steps should remove most or all of these DOC: Michael Meyer’s “inclusive” tier of analytical methods is dropped from this version. This and other tiering match tables will be included as csv files SDD: Matt Brousil’s “unlikely” grouping for sampling methods is dropped from this version. The same grouping is dropped from collection equipment as well TSS: Katie Willi’s “ambiguous” tiers for analytical methods are dropped from this version 8.0.1.2 Differences between the the two larger datasets: MR’s dataset (simul) and our attempt at recreating it (simultaneous_data) do not match in size. While investigating the reason behind this, MRB found ~1.4 million site-date-parameter combinations in MR’s wqp_lagos_unity.csv download, after removing LAGOS, that we don’t have in our raw WQP download using the current workflow method. Of those 1.4 million combos we don’t have the parameter breakdown is: chla 34k, doc 33k sdd 92k, tss 1.2 million. Within the 1.4 million missing combinations, 7.7k - 15.1k are sites we don’t have at all in our WQP download. This range is due to some uncertainty from us intentionally dropping one of the tss CharacteristicNames. We’re unfortunately limited in how much we can dig into the dataset differences further. It would take more time to understand how the original AquaSat’s WQP querying process differs from the USGS method we’ve based our pipeline on. The original dataset downloaded for MR’s version has also been processed to the point where many columns that would help with comparing versions have been dropped. KW did a quick download of the four parameters from WQP using her own method and found that her number of records still differed from MR’s. tldr: We tried to create a perfect, updated match to MR’s simul dataset but hit a wall. 8.0.1.3 Maps of current records MR’s original dataset: Code simul_vis_gg_mr &lt;- params$simul_vis_gg_mr Our dataset: Code simul_vis_gg &lt;- params$simul_vis_gg Our strict dataset: Code simul_vis_gg_strict &lt;- params$simul_vis_gg_strict 8.0.2 Comparisons against MR’s AquaSat 1 version: 8.0.2.1 Parameter distributions MR’s original dataset: Our dataset: Our strict dataset: Comparison table: parameter dataset mean_val sd_val chla Current dataset 22.38 38.95 chla MR simul 19.74 33.55 chla Strict dataset 18.95 32.43 doc Current dataset 11.02 9.04 doc MR simul 8.59 7.85 doc Strict dataset 10.54 8.89 secchi Current dataset 0.92 0.86 secchi MR simul 0.95 0.78 secchi Strict dataset 0.94 0.80 tss Current dataset 20.80 32.08 tss MR simul 21.02 30.29 tss Strict dataset 19.62 31.68 8.0.3 Modeling We’ll run the linear model with all three versions of the dataset. The code below is adapted from MR’s scripts. Code range_val &lt;- 234 # Multiply chla by 234 nap_test &lt;- map(.x = list(simul = rename(simul, chla = chl_a), simultaneous_data = simultaneous_data, simultaneous_data_strict = simultaneous_data_strict), .f = ~ expand_grid(.x, chl_ratio = range_val) %&gt;% mutate(power = ifelse(chl_ratio == 234, 0.57, 1), chla_biomass = exp(log(chl_ratio / 1000) + log(chla) * power), tss_dead = tss - chla_biomass)) ## Warning in log(chla): NaNs produced ## Warning in log(chla): NaNs produced Code map(.x = nap_test, .f = ~ .x %&gt;% #Remove negatives and very small numbers (ug/L of sediment is basically zero) filter(tss_dead &gt; 0.001) %&gt;% # sample_frac(0.1) %&gt;% ggplot(., aes(chla,tss_dead,color = type)) + facet_wrap(~chl_ratio) + geom_point() + scale_x_log10() + scale_y_log10() + stat_poly_eq() + ggthemes::theme_few() + scale_color_manual(values = c(&#39;seagreen3&#39;,&#39;skyblue3&#39;,&#39;saddlebrown&#39;))) ## $simul ## ## $simultaneous_data ## Warning: [1m[22mTransformation introduced infinite values in continuous x-axis ## [1m[22mTransformation introduced infinite values in continuous x-axis ## Warning: [1m[22mRemoved 132 rows containing non-finite values ## (`stat_poly_eq()`). ## ## $simultaneous_data_strict ## Warning: [1m[22mTransformation introduced infinite values in continuous x-axis ## Warning: [1m[22mTransformation introduced infinite values in continuous x-axis ## Warning: [1m[22mRemoved 18 rows containing non-finite values (`stat_poly_eq()`). Code nap_est &lt;- map(.x = nap_test, .f = ~ .x %&gt;% filter(tss_dead &gt; 0.001) %&gt;% mutate(secchi = ifelse(secchi &lt; 0.01, 0.01, secchi), kd = (1 / secchi))) Code model &lt;- map(.x = nap_est, .f = ~ lm(kd ~ tss_dead + doc + chla, data = .x)) nap_resid &lt;- map2(.x = nap_est, .y = model, .f = ~ .x %&gt;% mutate(residuals = .y$residuals, pred = .y$fitted.values)) model_metrics &lt;- pmap_df(.l = list(nap_resid = nap_resid, name = names(nap_resid), model = model), .f = function(nap_resid, name, model){ dataset &lt;- name rmse &lt;- round(rmse(nap_resid$kd, nap_resid$pred), 2) adj_rsq &lt;- round(unlist(summary(model)[&quot;adj.r.squared&quot;][1]), 2) tibble(dataset = dataset, rmse = rmse, adj_rsq = adj_rsq) }) model_metrics %&gt;% left_join(x = ., y = tibble( dataset = c(&quot;simul&quot;, &quot;simultaneous_data&quot;, &quot;simultaneous_data_strict&quot;), rows = c(nrow(simul), nrow(simultaneous_data), nrow(simultaneous_data_strict))), by = c(&quot;dataset&quot;)) %&gt;% kable() %&gt;% kable_paper() dataset rmse adj_rsq rows simul 1.96 0.16 100853 simultaneous_data 1.85 0.23 55675 simultaneous_data_strict 1.32 0.36 25588 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
